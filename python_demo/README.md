# Overview

- Kafka demo: receives and parses kafka records sent by ape_dts in avro format.

- Http demo: fetches and parses records from ape_dts HTTP server in avro format.

- redis_demo: demo function to update redis by received records.

- elasticsearch_demo: demo function to update elasticsearch by received records.

# Quick Start
## Kafka demo
- 1, start [ape_dts task](https://github.com/apecloud/cubetran-core/blob/main/docs/en/tutorial/mysql_to_kafka_consumer.md) to send data to Kafka.
- 2, install dependent packages in requirements.txt.
- 3, update configs in kafka_consumer_demo.py.
- 4, update function/function.py to handle received records.
```
import log

def handle(record, config):
    log.log("received record: " + str(record))
```
- 5, if there were any records sent by ape_dts, you may see logs in log/default.log:
```
2024-10-11 18:26:00,822 - INFO - received record: {'schema': 'test_db_1', 'tb': '', 'operation': 'ddl', 'fields': None, 'before': None, 'after': None, 'extra': {'query': 'create table a(id int, value int)', 'ddl_type': 'create_table', 'db_type': 'mysql'}}

2024-10-11 18:26:00,826 - INFO - received record: {'schema': 'test_db_1', 'tb': 'a', 'operation': 'insert', 'fields': [{'name': 'id', 'column_type': 'int', 'avro_type': 'Long'}, {'name': 'value', 'column_type': 'int', 'avro_type': 'Long'}], 'before': None, 'after': {'value': 2, 'id': 2}, 'extra': None}

2024-10-11 18:26:00,831 - INFO - received record: {'schema': 'test_db_1', 'tb': '', 'operation': 'ddl', 'fields': None, 'before': None, 'after': None, 'extra': {'ddl_type': 'drop_table', 'db_type': 'mysql', 'query': 'DROP TABLE `a` /* generated by server */'}}
```

## Http demo
- 1, start [ape_dts task](https://github.com/apecloud/cubetran-core/blob/main/docs/en/tutorial/mysql_to_http_server_consumer.md) as Http server.
- 2, update configs in http_consumer_demo.py.
- 3, when fetched records from ape_dts, the outputs are same as the Kafka demo.