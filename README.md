# Overview
This is a demo project which provides:
- a python client to receive and parse kafka records which sent by ape_dts in avro format.
- demo functions to update redis/elasticsearch by received records.

# Quick Start
- 1, install dependent packages in requirements.txt.
- 2, update function/config.ini.
- 3, update function/function.py to handle received records:
```
import log

def handle(record, config):
    if config.get("print_record") != None and bool(config.get("print_record")):
        log.log("received record: " + str(record))
```
- 4, execute index.py to receive kafka records.
- 5, if there were any records sent by ape_dts, you may see logs in log/default.log:
```
2024-10-11 18:26:00,822 - INFO - received record: {'schema': 'test_db_1', 'tb': '', 'operation': 'ddl', 'fields': None, 'before': None, 'after': None, 'extra': {'query': 'create table a(id int, value int)', 'ddl_type': 'create_table', 'db_type': 'mysql'}}

2024-10-11 18:26:00,826 - INFO - received record: {'schema': 'test_db_1', 'tb': 'a', 'operation': 'insert', 'fields': [{'name': 'id', 'column_type': 'int', 'avro_type': 'Long'}, {'name': 'value', 'column_type': 'int', 'avro_type': 'Long'}], 'before': None, 'after': {'value': 2, 'id': 2}, 'extra': None}

2024-10-11 18:26:00,831 - INFO - received record: {'schema': 'test_db_1', 'tb': '', 'operation': 'ddl', 'fields': None, 'before': None, 'after': None, 'extra': {'ddl_type': 'drop_table', 'db_type': 'mysql', 'query': 'DROP TABLE `a` /* generated by server */'}}
```
